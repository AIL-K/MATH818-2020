@article{Kitagawa.2018, 
title = {{Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice}}, 
author = {Kitagawa, Toru and Tetenov, Aleksey}, 
journal = {Econometrica}, 
issn = {0012-9682}, 
doi = {10.3982/ecta13288}, 
abstract = {{One of the main objectives of empirical analysis of experiments and quasi‐experiments is to inform policy decisions that determine the allocation of treatments to individuals with different observable covariates. We study the properties and implementation of the Empirical Welfare Maximization (EWM) method, which estimates a treatment assignment policy by maximizing the sample analog of average social welfare over a class of candidate treatment policies. The EWM approach is attractive in terms of both statistical performance and practical implementation in realistic settings of policy design. Common features of these settings include: (i) feasible treatment assignment rules are constrained exogenously for ethical, legislative, or political reasons, (ii) a policy maker wants a simple treatment assignment rule based on one or more eligibility scores in order to reduce the dimensionality of individual observable characteristics, and/or (iii) the proportion of individuals who can receive the treatment is a priori limited due to a budget or a capacity constraint. We show that when the propensity score is known, the average social welfare attained by EWM rules converges at least at n−1/2 rate to the maximum obtainable welfare uniformly over a minimally constrained class of data distributions, and this uniform convergence rate is minimax optimal. We examine how the uniform convergence rate depends on the richness of the class of candidate decision rules, the distribution of conditional treatment effects, and the lack of knowledge of the propensity score. We offer easily implementable algorithms for computing the EWM rule and an application using experimental data from the National JTPA Study.}}, 
pages = {591--616}, 
number = {2}, 
volume = {86}, 
note = {넘모 어렵당}, 
keywords = {Empirical Welfare Maximization,Treatment Rule}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2018/Kitagawa_2018_Econometrica_Who%20Should%20Be%20Treated-%20Empirical%20Welfare%20Maximization%20Methods%20for%20Treatment%20Choice.pdf}, 
year = {2018}
}
@article{uze, 
title = {{swaminathan15.pdf}}, 
author = {}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/swaminathan15.pdf}
}
@article{Manski.2004, 
title = {{Statistical Treatment Rules for Heterogeneous Populations}}, 
author = {Manski, Charles F.}, 
journal = {Econometrica}, 
issn = {1468-0262}, 
doi = {10.1111/j.1468-0262.2004.00530.x}, 
abstract = {{An important objective of empirical research on treatment response is to provide decision makers with information useful in choosing treatments. This paper studies minimax-regret treatment choice using the sample data generated by a classical randomized experiment. Consider a utilitarian social planner who must choose among the feasible statistical treatment rules, these being functions that map the sample data and observed covariates of population members into a treatment allocation. If the planner knew the population distribution of treatment response, the optimal treatment rule would maximize mean welfare conditional on all observed covariates. The appropriate use of covariate information is a more subtle matter when only sample data on treatment response are available. I consider the class of conditional empirical success rules; that is, rules assigning persons to treatments that yield the best experimental outcomes conditional on alternative subsets of the observed covariates. I derive a closed-form bound on the maximum regret of any such rule. Comparison of the bounds for rules that condition on smaller and larger subsets of the covariates yields sufficient sample sizes for productive use of covariate information. When the available sample size exceeds the sufficiency boundary, a planner can be certain that conditioning treatment choice on more covariates is preferable (in terms of minimax regret) to conditioning on fewer covariates.}}, 
pages = {1221--1246}, 
number = {4}, 
volume = {72}, 
keywords = {Treatment Rule}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2004/Manski_2004_Econometrica_Statistical%20Treatment%20Rules%20for%20Heterogeneous%20Populations.pdf}, 
year = {2004}
}
@article{Demirer.2019, 
title = {{Semi-Parametric Efficient Policy Learning with Continuous Actions}}, 
author = {Demirer, Mert and Syrgkanis, Vasilis and Lewis, Greg and Chernozhukov, Victor}, 
journal = {arXiv.org}, 
url = {arXiv.org}, 
abstract = {{We consider off-policy evaluation and optimization with continuous action spaces. We focus on observational data where the data collection policy is unknown and needs to be estimated. We take a semi-parametric approach where the value function takes a known parametric form in the treatment, but we are agnostic on how it depends on the observed contexts. We propose a doubly robust off-policy estimate for this setting and show that off-policy optimization based on this estimate is robust to estimation errors of the policy function or the regression model. Our results also apply if the model does not satisfy our semi-parametric form, but rather we measure regret in terms of the best projection of the true value function to this functional space. Our work extends prior approaches of policy optimization from observational data that only considered discrete actions. We provide an experimental evaluation of our method in a synthetic data example motivated by optimal personalized pricing and costly resource allocation.}}, 
keywords = {Policy Learning}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2019/Demirer_2019_arXiv.org_Semi-Parametric%20Efficient%20Policy%20Learning%20with%20Continuous%20Actions.pdf}, 
year = {2019}, 
rating = {0}
}
@article{Zhou.2017, 
title = {{Residual Weighted Learning for Estimating Individualized Treatment Rules}}, 
author = {Zhou, Xin and Mayer-Hamblett, Nicole and Khan, Umer and Kosorok, Michael R.}, 
journal = {Journal of the American Statistical Association}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.2015.1093947}, 
pmid = {28943682}, 
abstract = {{Personalized medicine has received increasing attention among statisticians, computer scientists, and clinical practitioners. A major component of personalized medicine is the estimation of individualized treatment rules (ITRs). Recently, Zhao et al. proposed outcome weighted learning (OWL) to construct ITRs that directly optimize the clinical outcome. Although OWL opens the door to introducing machine learning techniques to optimal treatment regimes, it still has some problems in performance. (1) The estimated ITR of OWL is affected by a simple shift of the outcome. (2) The rule from OWL tries to keep treatment assignments that subjects actually received. (3) There is no variable selection mechanism with OWL. All of them weaken the finite sample performance of OWL. In this article, we propose a general framework, called residual weighted learning (RWL), to alleviate these problems, and hence to improve finite sample performance. Unlike OWL which weights misclassification errors by clinical outcomes, RWL weights these errors by residuals of the outcome from a regression fit on clinical covariates excluding treatment assignment. We use the smoothed ramp loss function in RWL and provide a difference of convex (d.c.) algorithm to solve the corresponding nonconvex optimization problem. By estimating residuals with linear models or generalized linear models, RWL can effectively deal with different types of outcomes, such as continuous, binary, and count outcomes. We also propose variable selection methods for linear and nonlinear rules, respectively, to further improve the performance. We show that the resulting estimator of the treatment rule is consistent. We further obtain a rate of convergence for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of the proposed RWL methods is illustrated in simulation studies and in an analysis of cystic fibrosis clinical trial data. Supplementary materials for this article are available online.}}, 
pages = {169--187}, 
number = {517}, 
volume = {112}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2017/Zhou_2017_Journal%20of%20the%20American%20Statistical%20Association_Residual%20Weighted%20Learning%20for%20Estimating%20Individualized%20Treatment%20Rules.pdf}, 
year = {2017}
}
@article{Athey.2017txo, 
title = {{Policy Learning with Observational Data}}, 
author = {Athey, Susan and Wager, Stefan}, 
journal = {arXiv}, 
eprint = {1702.02896}, 
abstract = {{In many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisfies application-specific constraints, such as budget, fairness, simplicity, or other functional form constraints. For example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. We propose a new approach to this problem motivated by the theory of semiparametrically efficient estimation. Our method can be used to optimize either binary treatments or infinitesimal nudges to continuous treatments, and can leverage observational data where causal effects are identified using a variety of strategies, including selection on observables and instrumental variables. Given a doubly robust estimator of the causal effect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.}}, 
note = {수학이 너무 어렵당ㅋㅋ
더 간단한 모델로 알고리즘부터 배워도 될까...
treatment assign 문제가 뭔지는 알겠다}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2017/Athey_2017_arXiv_Policy%20Learning%20with%20Observational%20Data.pdf}, 
year = {2017}
}
@article{Kallus.2018, 
title = {{Policy Evaluation and Optimization with Continuous Treatments}}, 
author = {Kallus, Nathan and Zhou, Angela}, 
journal = {arXiv}, 
eprint = {1802.06037}, 
abstract = {{We study the problem of policy evaluation and learning from batched contextual bandit data when treatments are continuous, going beyond previous work on discrete treatments. Previous work for discrete treatment/action spaces focuses on inverse probability weighting (IPW) and doubly robust (DR) methods that use a rejection sampling approach for evaluation and the equivalent weighted classification problem for learning. In the continuous setting, this reduction fails as we would almost surely reject all observations. To tackle the case of continuous treatments, we extend the IPW and DR approaches to the continuous setting using a kernel function that leverages treatment proximity to attenuate discrete rejection. Our policy estimator is consistent and we characterize the optimal bandwidth. The resulting continuous policy optimizer (CPO) approach using our estimator achieves convergent regret and approaches the best-in-class policy for learnable policy classes. We demonstrate that the estimator performs well and, in particular, outperforms a discretization-based benchmark. We further study the performance of our policy optimizer in a case study on personalized dosing based on a dataset of Warfarin patients, their covariates, and final therapeutic doses. Our learned policy outperforms benchmarks and nears the oracle-best linear policy.}}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2018/Kallus_2018_arXiv_Policy%20Evaluation%20and%20Optimization%20with%20Continuous%20Treatments.pdf}, 
year = {2018}
}
@article{Foster.2019, 
title = {{Orthogonal Statistical Learning}}, 
author = {Foster, Dylan J and Syrgkanis, Vasilis}, 
journal = {arXiv}, 
eprint = {1901.09036}, 
abstract = {{We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target parameter and one for the nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate settings in which the target parameter belongs to a complex nonparametric class. We provide conditions on the metric entropy of the nuisance and target classes such that oracle rates---rates of the same order as if we knew the nuisance parameter---are achieved. We also derive new rates for specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data.}}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2019/Foster_2019_arXiv_Orthogonal%20Statistical%20Learning.pdf}, 
year = {2019}
}
@article{Zhou.2018, 
title = {{Offline Multi-Action Policy Learning: Generalization and Optimization}}, 
author = {Zhou, Zhengyuan and Athey, Susan and Wager, Stefan}, 
journal = {arXiv}, 
eprint = {1810.04778}, 
abstract = {{In many settings, a decision-maker wishes to learn a rule, or policy, that maps from observable characteristics of an individual to an action. Examples include selecting offers, prices, advertisements, or emails to send to consumers, as well as the problem of determining which medication to prescribe to a patient. While there is a growing body of literature devoted to this problem, most existing results are focused on the case where data comes from a randomized experiment, and further, there are only two possible actions, such as giving a drug to a patient or not. In this paper, we study the offline multi-action policy learning problem with observational data and where the policy may need to respect budget constraints or belong to a restricted policy class such as decision trees. We build on the theory of efficient semi-parametric inference in order to propose and implement a policy learning algorithm that achieves asymptotically minimax-optimal regret. To the best of our knowledge, this is the first result of this type in the multi-action setup, and it provides a substantial performance improvement over the existing learning algorithms. We then consider additional computational challenges that arise in implementing our method for the case where the policy is restricted to take the form of a decision tree. We propose two different approaches, one using a mixed integer program formulation and the other using a tree-search based algorithm.}}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2018/Zhou_2018_arXiv_Offline%20Multi-Action%20Policy%20Learning-%20Generalization%20and%20Optimization.pdf}, 
year = {2018}
}
@article{Dimakopoulou.2017, 
title = {{Estimation Considerations in Contextual Bandits}}, 
author = {Dimakopoulou, Maria and Zhou, Zhengyuan and Athey, Susan and Imbens, Guido}, 
journal = {arXiv}, 
eprint = {1711.07077}, 
abstract = {{Contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used, particularly in the presence of rich heterogeneity or complex outcome models, which can lead to difficult estimation problems along the path of learning. We study a consideration for the exploration vs. exploitation framework that does not arise in multi-armed bandits but is crucial in contextual bandits; the way exploration and exploitation is conducted in the present affects the bias and variance in the potential outcome model estimation in subsequent stages of learning. We develop parametric and non-parametric contextual bandits that integrate balancing methods from the causal inference literature in their estimation to make it less prone to problems of estimation bias. We provide the first regret bound analyses for contextual bandits with balancing in the domain of linear contextual bandits that match the state of the art regret bounds. We demonstrate the strong practical advantage of balanced contextual bandits on a large number of supervised learning datasets and on a synthetic example that simulates model mis-specification and prejudice in the initial training data. Additionally, we develop contextual bandits with simpler assignment policies by leveraging sparse model estimation methods from the econometrics literature and demonstrate empirically that in the early stages they can improve the rate of learning and decrease regret.}}, 
keywords = {Contextual Bandits,Machine Learning}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2017/Dimakopoulou_2017_arXiv_Estimation%20Considerations%20in%20Contextual%20Bandits.pdf}, 
year = {2017}
}
@article{Zhao.2012, 
title = {{Estimating Individualized Treatment Rules Using Outcome Weighted Learning}}, 
author = {Zhao, Yingqi and Zeng, Donglin and Rush, A. John and Kosorok, Michael R.}, 
journal = {Journal of the American Statistical Association}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.2012.695674}, 
pmid = {23630406}, 
abstract = {{There is increasing interest in discovering individualized treatment rules (ITRs) for patients who have heterogeneous responses to treatment. In particular, one aims to find an optimal ITR that is a deterministic function of patient-specific characteristics maximizing expected clinical outcome. In this article, we first show that estimating such an optimal treatment rule is equivalent to a classification problem where each subject is weighted proportional to his or her clinical outcome. We then propose an outcome weighted learning approach based on the support vector machine framework. We show that the resulting estimator of the treatment rule is consistent. We further obtain a finite sample bound for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of the proposed approach is demonstrated via simulation studies and an analysis of chronic depression data.}}, 
pages = {1106--1118}, 
number = {499}, 
volume = {107}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2012/Zhao_2012_Journal%20of%20the%20American%20Statistical%20Association_Estimating%20Individualized%20Treatment%20Rules%20Using%20Outcome%20Weighted%20Learning.pdf}, 
year = {2012}
}
@article{Dudik.2011, 
title = {{Doubly Robust Policy Evaluation and Learning}}, 
author = {Dudik and Langford and Li}, 
keywords = {Machine Learning,policy evaluation}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2011/dudiklangfordli2011.pdf}, 
year = {2011}
}
@article{Krishnamurthy.2019, 
title = {{Contextual Bandits with Continuous Actions: Smoothing, Zooming, and Adapting}}, 
author = {Krishnamurthy, Akshay and Langford, John and Slivkins, Aleksandrs and Zhang, Chicheng}, 
journal = {arXiv}, 
eprint = {1902.01520}, 
abstract = {{We study contextual bandit learning with an abstract policy class and continuous action space. We obtain two qualitatively different regret bounds: one competes with a smoothed version of the policy class under no continuity assumptions, while the other requires standard Lipschitz assumptions. Both bounds exhibit data-dependent "zooming" behavior and, with no tuning, yield improved guarantees for benign problems. We also study adapting to unknown smoothness parameters, establishing a price-of-adaptivity and deriving optimal adaptive algorithms that require no additional information.}}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/2019/Krishnamurthy_2019_arXiv_Contextual%20Bandits%20with%20Continuous%20Actions-%20Smoothing,%20Zooming,%20and%20Adapting.pdf}, 
year = {2019}
}
@article{dhn, 
title = {{15732\_OnlineAppendix.pdf}}, 
author = {}, 
local-url = {file://localhost/Users/wonjun/Library/Mobile%20Documents/com~apple~CloudDocs/PapersLibrary/15732_OnlineAppendix.pdf}
}